# ğŸ¡ Pinica IA â€“ Intelligent Home Assistant with Modular Agents

> **Local-first smart home system** with voice, vision, and language intelligence.  
> Your house **listens, sees, understands, speaks, and searches for you.**

---

## âœ¨ Overview

`pinica_ia` is a modular and agent-based home automation framework that integrates:

- ğŸ™ï¸ Real-time speech transcription with Whisper
- ğŸ‘ï¸ Vision detection using YOLOv8 (via OpenCV)
- ğŸ§  Local LLM-based reasoning using Ollama + Phi-4
- ğŸ—£ï¸ Natural speech responses using Facebook MMS-TTS
- ğŸŒ Web search agent via DuckDuckGo
- ğŸ’¡ Device control via MQTT (in development)
- ğŸ“Š Optional UI with Streamlit (in development)

---

## ğŸ§  Agent Architecture

Each function is encapsulated as an **independent agent**, orchestrated asynchronously:

```
[ğŸ™ï¸ AudioAgent] â†’ Captures & transcribes speech
         â”‚
         â–¼
[ğŸ§  IntentClassifierAgent] â†’ Detects wake word + classifies intent
         â”‚
 â”Œâ”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
 â–¼                            â–¼
[ğŸ‘ï¸ VisionAgent]       [ğŸŒ SearchAgent]
         â”‚                     â”‚
         â–¼                     â–¼
     Scene summary        Web search results
         â”‚                     â”‚
         â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â–¼
           [ğŸ§  LLMAgent] â†’ Generates response
                â”‚
                â–¼
          [ğŸ—£ï¸ SpeechAgent] â†’ Speaks back
```

---

## ğŸ“¦ Core Components

| Agent/Class            | Description                                                                 |
|------------------------|-----------------------------------------------------------------------------|
| `AudioAgent`           | Captures microphone audio and transcribes using Whisper                    |
| `VisionAgent`          | Detects people/objects via camera and describes scene with YOLOv8          |
| `LLMAgent`             | Handles user interaction via prompt + context using Ollama (Phi-4, etc.)   |
| `SpeechAgent`          | Converts response text into audio using Facebook MMS-TTS                   |
| `SearchAgent`          | Searches the internet with DuckDuckGo and returns structured results       |
| `IntentClassifierAgent`| Uses a local LLM to detect if input has "coca" and which action to perform |
| `main_async.py`        | Async orchestration and command pipeline                                   |

---

## ğŸš€ Getting Started

### 1. Clone the repository

```bash
git clone https://github.com/yourname/pinica_ia.git
cd pinica_ia
```

### 2. Install requirements

```bash
pip install -r requirements.txt
```

### 3. Start support services (if using Docker)

```bash
docker-compose up -d
```

### 4. Configure your environment

Edit `configs/rooms.json`:

```json
{
  "living_room": {
    "mic_device": 1,
    "cameras": ["rtsp://user:password@192.168.1.10:554/stream1"],
    "mqtt_topic": "home/living_room"
  }
}
```

And update `configs/secrets.json` with your credentials.

### 5. Run the assistant

```bash
python app/main_async.py
```

---

## ğŸ—£ï¸ Example Spoken Commands

| Transcription Example                              | Resulting Action                                           |
|----------------------------------------------------|------------------------------------------------------------|
| â€œCoca, what's in the camera?â€                      | VisionAgent runs, takes image, describes people/scene      |
| â€œHey Koka, search the internet for NCDD companyâ€   | DuckDuckGo search executed + summary spoken                |
| â€œFinalize assistant, Cocaâ€                         | Assistant shuts down gracefully                           |

---

## ğŸ“‚ Project Structure

```
pinica_ia/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ main_async.py
â”‚   â”œâ”€â”€ agents/
â”‚   â”‚   â”œâ”€â”€ audio_agent.py
â”‚   â”‚   â”œâ”€â”€ vision_agent.py
â”‚   â”‚   â”œâ”€â”€ search_agent.py
â”‚   â”‚   â”œâ”€â”€ llm_agent.py
â”‚   â”‚   â”œâ”€â”€ speech_agent.py
â”‚   â”‚   â””â”€â”€ intent_classifier_agent.py
â”‚   â”œâ”€â”€ utils/
â”‚   â”‚   â”œâ”€â”€ vision_utils.py
â”‚   â”‚   â”œâ”€â”€ audio_utils.py
â”‚   â”‚   â”œâ”€â”€ search_util.py
â”‚   â”‚   â””â”€â”€ llm_utils.py
â”‚   â”œâ”€â”€ configs/
â”‚   â”‚   â”œâ”€â”€ rooms.json
â”‚   â”‚   â””â”€â”€ secrets.json
â”‚   â””â”€â”€ search_logs/
â”œâ”€â”€ notebooks/
â”œâ”€â”€ docker-compose.yml
â””â”€â”€ requirements.txt
```

---

## ğŸ” Privacy & Wake Word

- Assistant only responds if a variation of â€œcocaâ€ is heard (e.g., â€œkokaâ€, â€œcokaâ€, â€œkoukaâ€).
- All processing is done locally unless a web search is required.
- Searches and audio logs are saved in separate folders for auditability.

---

## ğŸ§  Powered By

- [Whisper](https://github.com/openai/whisper)
- [Ollama](https://ollama.com/)
- [YOLOv8](https://github.com/ultralytics/ultralytics)
- [DuckDuckGo Search API](https://duckduckgo.com)
- [MMS-TTS](https://github.com/facebookresearch/fairseq/tree/main/examples/mms)

---

## ğŸ“˜ License

MIT Â© 2025 â€“ Developed by Niels & contributors